# Notes
## Demo
### Instructions
Collect: 
- Video. 
  - Audio channel: room. 
- Screen recording of Music X Machine. 
- Log output of ascii panel. 
- Audio channel: mic in. 
- Audio channel: Teo out. (NOT NOW)
- Audio channel: Flute out. (NOT NOW)
- Audio channel: Piano out. (NOT NOW)
- Audio channel: Music X Machine out. (NOT NOW)

### Scripted
- The script is generated by ChatGPT with very minor modifications. 
  - Modifications: edit generated responses. 
  - Guidance: responses are edited step-by-step. 
  - Human hinting: the human script writer uses USER to give short hints to GPT. 
  - All the above are kept to the minimum to unsure the chat is primarily written by GPT. 

## Features
- auto-retry when triple quotes unclosed / function call signature wrong. 
- Decide how much text to batch for t2s according to how much audio is in the output buffer and how long the next t2s is estimated to take.
- an online-learning linear model that predicts the compute time of t2s. 
- GPT streams token-by-token to let Teo start talking earlier. 

## Important
- Add references.txt to any publications.

## TODO
- music performance evaluation
- buy (wearable) microphone that effectively isolate the speaker

## Eventual TODO
- Try removing specific educational instructions from the system principles. See GPT can figure things out on itself. 

## ideas
- functions can take a bool as a param to decide if it's called before or after the words are uttered. 

## things to do because GPT is not well-principled
- let start_session trigger on the next wait()?
- auto-interrupt whenever GPT talks during a session?
- consecutive same-function calls should be redacted from history, to prevent in-context learning.
- Observation: even with verbose sys principles about waiting at commit 9e7369c1e9d7d871336eaa0f8aaf72b4b6868e13, GPT still sometimes gives an empty response instead of calling the wait function. 
  - So I'm adding a special rule to parse the empty response as wait. 

## far todo
- allow interruption
- multi-choice in-rule selection

## Misc.
What we need is to minimize the perplexity of the ideal chat. 
There is 0 computation overhead. 
Too bad openAI doesn't provide that. 

There are many ways to do it that makes sense. However, two requirements are in contradiction:
- Interaction with the user should be top-level. 
  - If, alternatively, we let GPT say "tell the student 'You did well'", that will be too different from the RLHF training, and may affect interaction quality. 
- GPT should be able to discern different agents' messages sent to it. 

No need to expose too many agents to GPT. To GPT, "I" can just be Music X Machine. 

number of round trips should be minimized. prompting is better than prompt injection. 

Experiment conclusion: Adopt a system persona but chatting as USER. Relay the true user's words in quotes.

Experiment conclusion: (1) OpenAI function seems better than custom enum solutions. (2) 没事别在对话中乱用 SYSTEM. 

Experiment conclusion: GPT will read previous function calls in the history. Man-in-middle Changes to the call history does intervene GPT in the expected way. 

Experiment conclusion: query GPT multiple times consecutively to allow combo. Use sys principles like "Hint: At any point you decide it's best to do nothing and wait, explicitly call the provided function. Carefully examine your previous responses to know what you have done and what you should do next."

Question: should we use GPT's new image understanding ability to process user performance (instead of MIDI)? Any chance to train an adaptor?

Yixiao: maybe: whether to wait is a must-call function every iteration. (e.g. output a json.) This is just like VisualChatGPT. 

Observation: even when sys principles tell GPT otherwise, GPT often repeats itself (even in two back-to-back responses). Could it be related to the fact that no USER entry is inbetween, which deviates from RLHF? 

We should try to perfect the sys principles using test_ideal WITHOUT example. When deployed, provide examples. 

10/17 commit a6628881543e140ab3e6ef7ae9034e448c7c132c measurement
    sys principle: 2700 tokens + ideal chat: 2300 tokens

FastSpeech2 can finally run in real-time with 3050Ti.  

## next
- https://platform.openai.com/docs/guides/speech-to-text/improving-reliability
  - To solve the proper noun problem (e.g. "KR feedback" "GPT"), OpenAI recommends: post-process speech2text with GPT4. We can do that for free! I even fell no extra prompt is needed. 
- When input audio contains only music, the transcription model freaks out, sometimes using natural language to hallucinate the thoughts of the student. Solution: append each audio input with "here it goes" and remove it in the output. 
  - Weird result: sometimes the prepended audio is not transcribed. The model can somehow tell these are different segments and only transcribe the latter one. Maybe related to dirty training data containing untranscribed headers. 
